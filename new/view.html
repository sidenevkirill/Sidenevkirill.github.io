<html lang="ru" dir="auto"><head><meta charset="utf-8"><meta http-equiv="x-ua-compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name="robots" content="index, follow"><title>Просмотер</title><meta name="keywords" content=""><meta name="description" content="Second part of the Introduction to the world of Reinforcement Learning, where I cover some more advanced deep RL algorithms and ideas in the space."><meta name="author" content="Johann Gerberding"><link rel="canonical" href="https://johanngerberding.github.io/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/"><link crossorigin="anonymous" href="https://johanngerberding.github.io/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style"><script defer="" crossorigin="anonymous" src="/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js" integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload="hljs.initHighlightingOnLoad()"></script>
<link rel="icon" href="https://johanngerberding.github.io/favicon.ico"><link rel="icon" type="image/png" sizes="16x16" href="https://johanngerberding.github.io/favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="https://johanngerberding.github.io/favicon-32x32.png"><link rel="apple-touch-icon" href="https://johanngerberding.github.io/apple-touch-icon.png"><link rel="mask-icon" href="https://johanngerberding.github.io/safari-pinned-tab.svg"><meta name="theme-color" content="#2e2e33"><meta name="msapplication-TileColor" content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="A Peek into Deep Reinforcement Learning - Part II"><meta property="og:description" content="Second part of the Introduction to the world of Reinforcement Learning, where I cover some more advanced deep RL algorithms and ideas in the space."><meta property="og:type" content="article"><meta property="og:url" content="https://johanngerberding.github.io/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-14T10:00:00+02:00"><meta property="article:modified_time" content="2022-03-14T10:00:00+02:00"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="A Peek into Deep Reinforcement Learning - Part II"><meta name="twitter:description" content="Second part of the Introduction to the world of Reinforcement Learning, where I cover some more advanced deep RL algorithms and ideas in the space."><script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://johanngerberding.github.io/posts/"},{"@type":"ListItem","position":2,"name":"A Peek into Deep Reinforcement Learning - Part II","item":"https://johanngerberding.github.io/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/"}]}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Peek into Deep Reinforcement Learning - Part II","name":"A Peek into Deep Reinforcement Learning - Part II","description":"Second part of the Introduction to the world of Reinforcement Learning, where I cover some more advanced deep RL algorithms and ideas in the space.","keywords":[],"articleBody":" In this second part on Deep Reinforcement Learning we are going to explore some of the combined methods (look into Part I if you don't know what I mean with combined methods). We will start with the so called Advantage Actor-Critic algorithm which combines some of the concepts we have learned about in the first part. Then we dive into the very popular Proximal Policy Optimization algorithm. Moreover I will give you an overview of the Alpha(Go)Zero algorithm from DeepMind. In the third part of this series we are going to talk more about Imitation Learning and Model-based RL. Advantage Actor-Critic (A2C) As mentioned before, Advantage Actor-Critic (A2C) algorithms combine the ideas from policy gradient methods (e.g. REINFORCE) and a learned value function (e.g. DQN). Here we reinforce a policy with a learned reinforcing signal generated by a learned value function. A2C algorithms therefore consist of two jointly learned components: an actor which learns a parameterized policy and a critic which learns a value function to evaluate state-action pairs (it provides a reinforcing signal to the actor) The motivation for this is that a learned reinforcing signal can be much more informative for a policy than the rewards available from an environment. Instead of learning $Q^{\\pi}(s,a)$ or $V^{\\pi}$, it is common to learn the so called advantage function $A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$ as the reinforcing signal. The key idea behind this is that it is better to select an action based on how it performs relative to the other actions available in a particular state, instead of using the absolute value (hence the name advantage actor-critic). The actors learn a parameterized policy $\\pi_{\\theta}$ using the policy gradient. This is similar to the REINFORCE algorithm, but instead of the Monte-Carlo estimate $R_{t}(\\tau)$, the advantage is used: $$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{t} \\Big[ A_{t}^{\\pi} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t} | s_{t})\\Big] $$\nThe critic is responsible for learning how to evaluate state-action-pairs and using this to generate $A^{\\pi}$. To estimate the advante function, we will go over two possible methods: n-step returns and Generalized Advantage Estimation (GAE). In general the advantage function measures the extent to which an action is better or worse than the policy's average action in a particular state: $$ A^{\\pi}(s_{t},a_{t}) = Q^{\\pi}(s_{t},a_{t}) - V^{\\pi}(s_{t}) $$\nOne benefit of using the advantage instead of $Q^{\\pi}$ or $V^{\\pi}$ is that it avoids penalizing an action for the policy currently being in a particularly bad state, like in the following example: $$ Q^{\\pi}(s,a) = 110, \\quad V^{\\pi}(s) = 100, \\quad A^{\\pi}(s,a) = 10 $$\n$$ Q^{\\pi}(s,a) = -90, \\quad V^{\\pi}(s) = -100, \\quad A^{\\pi}(s,a) = 10 $$\nThe advantage function is better able to capture the long-term effects of an action because it considers all future time steps while ignoring the effects of all the actions to date. n-step Returns As seen before, to calculate $A^{\\pi}$ we need estimates for both $Q^{\\pi}(s,a)$ and $V^{\\pi}(s)$. In the n-step Returns method we achieve this by learning $V^{\\pi}(s)$ and estimating $Q^{\\pi}(s,a)$ from it: $$ Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim \\pi} \\Big[ r_{t} + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} … + \\gamma^{n} r_{t+n} \\Big] + \\gamma^{n+1} \\hat{V}^{\\pi}(s_{t+n+1}) $$\nThe expectation part of the Q-value estimate show above is calculated based on a 3-step return, which means that we use our collected trajectory data to look three steps in the future and sum up the rewards multiplied by a discounting factor $\\gamma^{t}$. This part of the equation is unbiased but has a high variance because it comes from only one trajectory. $n$ is a hyperparameter that needs to be tuned. The bigger the value of $n$, the higher the variance of the estimate. The return after the n-th step is calculated by the critic network. It has lower variance since it reflects an expectation over all of the trajectories seen so far, but it is biased because it is calculated using a function approximator. From this we now get a formula for estimating the advantage: $$ A_{NSTEP}^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t}) $$\n$$ A_{NSTEP}^{\\pi}(s_{t}, a_{t}) \\approx r_{t} + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + … + \\gamma^{n} r_{t+n} + \\gamma^{n+1} \\hat{V}^{\\pi}(s_{t+n+1}) - \\hat{V}^{\\pi}(s_{t}) $$\nGeneralized Advantage Estimation (GAE) Generalized Advantage Estimation was proposed as an improvement over the n-step returns estimate for the advantage function. It addresses the problem of having to explicitly choose the number of steps of returns $n$. The main idea is, instead of picking one value of $n$, we mix multiple values by calculating the advantage using a weighted average of individual advantages calculated with $n = 1, 2, 3, ..., k$. This significantly reduces the variance of the estimator while keeping the bias introduced as low as possible. $$ A_{GAE}^{\\pi}(s_{t}, a_{t}) = \\sum^{\\infty}_{l=0}(\\gamma \\lambda) \\delta_{t+l} $$\n$$ \\text{where } \\delta_{t} = r_{t} + \\gamma V^{\\pi}(s_{t+1}) - V^{\\pi}(s_{t}) $$\nGAE is taking a weighted average over a number of advantage estimators with different bias and variance. It weights the high-bias, low-variance 1-step advantage the most but also includes contributions from lower-bias, higher-variance estimators using $2,3,...,n$ steps. The contributions decay at an exponential rate as the number of steps increases and the decay rate gets controlled by the coefficient $\\lambda$ (the larger $\\lambda$, the higher the variance). In contrast to $n$, $\\lambda$ represents a softer choice than $n$, which means smaller values of $\\lambda$ will more heavily weight the V-function estimate, whilst larger values will weight the actual rewards more. Algorithm \u0026 Network Architecture But how do we learn the value function $V^{\\pi}$? We have different possibilities here, one of them is Temporal Difference Learning, similar to DQN. First we parameterize $V^{\\pi}$ with $\\theta$, then we generate $V_{tar}^{\\pi}$ for each of the experiences an agent gathers. Finally we minimize the distance between $\\hat{V}^{\\pi}(s; \\theta)$ and $V^{\\pi}_{tar}$ using a simple regression loss such as MSE. You can use different methods to generate $V_{tar}^{\\pi}$: n-step estimate: $$ V_{tar}^{\\pi}(s) = r + \\hat{V}^{\\pi}(s’; \\theta) $$\n$$ V_{tar}^{\\pi}(s_{t}) = r_{t} + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + … + \\gamma^{n} r_{t+n} + \\gamma^{n+1} \\hat{V}^{\\pi}(s_{t+n+1}) $$\nMonte-Carlo estimate: $$ V_{tar}^{\\pi}(s_{t}) = \\sum_{t’=t}^{T}\\gamma^{t’-t} r_{t’} $$\nGAE: $$ V_{tar}^{\\pi}(s_{t}) = A_{GAE}^{\\pi}(s_{t}, a_{t}) + \\hat{V}^{\\pi}(s_{t}) $$\nThe choice is often related to the method used to estimate the advantage. It is also possible to use a different, more complicated optimization procedure (trust region method) when learning the value function $\\hat{V}^{\\pi}$ which you can find in the original GAE paper. You can run the actor-critic algorithm online as well as batched, down below you see the simplified steps of an online version: take action $\\mathbf{a} \\sim \\pi_{\\theta}(\\mathbf{a}|\\mathbf{s})$, get $(\\mathbf{s}, \\mathbf{a}, \\mathbf{s’}, r)$ update $\\hat{V}_{\\phi}^{\\pi}$ using target $r + \\gamma \\hat{V}_{\\phi}^{\\pi}(\\mathbf{s’})$ evaluate $\\hat{A}^{\\pi}(\\mathbf{s}, \\mathbf{a}) = r(\\mathbf{s}, \\mathbf{a}) + \\gamma \\hat{V}_{\\phi}^{\\pi}(\\mathbf{s’}) - \\hat{V}_{\\phi}^{\\pi}(\\mathbf{s})$ $\\nabla_{\\theta} J(\\theta) \\approx \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}|\\mathbf{s})\\hat{A}^{\\pi}(\\mathbf{s}, \\mathbf{a})$ $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta} J(\\theta)$ repeat To learn the parameterized functions for the actor and the critic, you can use two separate neural networks. But it is also possible and conceptually appealing to use a network structure with shared parameters, because learning $\\pi$ and $V^{\\pi}(s)$ for the same task are related and they share the same input. Moreover your total number of learnable parameters is reduced which makes it overall more sample efficient. The sharing of the lower layers could be interpreted as learning a common representation of the state space. Upper layers are separate because the networks have different tasks, so their structure and size can be different. But sharing parameters has also some downsides: learning becomes more unstable because the two gradients can have different scales we need to balance the two which is typically accomplished by adding a scalar weight to one of the losses for scaling this adds another hyperparameter to tune Proximal Policy Optimization (PPO) Two of the main problems with policy gradient algorithms like REINFORCE are: susceptibility to performance collapse (agent suddenly performs bad) sample-inefficiency (on policy - no data reuse) The PPO paper addresses both of these issues by introducing a surrugate objective which avoids performance collapse by guarantueeing monotonic policy improvement and enables off-policy data reuse. This leads often to more stable and sample efficient training. PPO methods have some benefits of trust region policy optimization (TRPO) but are much simpler to implement, more general and have better sample efficiency. PPO can be used to extend f.e. REINFORCE of Actor-Critic methods by replacing their original objective $J(\\pi_{\\theta})$ with the surrogate objective. It is currently one of the most popular policy gradient algorithms. The paper presents two different variants of the objective function which we are diving into both. Adaptive KL Penalty This PPO version incorporates the Kullback-Leiber Divergence (KL) as a penalty into the surrogate objective. The KL is a measure for information loss between two distributions, you can find a nice explanation here. The objective function looks like the following: $$ J^{KPLEN}(\\theta) = \\max_{\\substack{\\theta}} \\mathbb{E}_{t}\\Big[r_{t}(\\theta)A_{t}-\\beta KL \\big(\\pi_{\\theta}(a_{t}|s_{t}) \\parallel \\pi_{\\theta_{old}}(a_{t}|s_{t})\\big)\\Big] $$\n$$ \\text{with } r(\\theta) = \\frac{\\pi(a_{t}|s_{t})}{\\pi_{old}(a_{t}|s_{t})} $$\n$\\beta$ is an adaptive coefficient which controls the size of the KL penalty (the larger $\\beta$, the larger the difference between $\\pi$ and $\\pi_{old}$). Because $\\beta$ is a new hyperparameter that has to work for different problems in different scenarios, it is not a constant value. Instead $\\beta$ gets recalculated after every policy update based on a heuristic which allows it to adapt over time: set target value $\\delta_{tar}$ for the expectation of KL, init $\\beta$ randomly use multiple epochs of minibatch SGD, optimize the KL-penalized surrogate objective compute $\\delta = \\mathbb{E}_{t}\\Big[ KL \\big(\\pi_{\\theta}(a_{t}|s_{t}) \\parallel \\pi_{\\theta_{old}}(a_{t}|s_{t})\\big) \\Big]$ if $\\delta \u003c \\delta_{tar}/1.5$ then $\\beta \\leftarrow \\beta/2$ else if $\\delta \u003e \\delta_{tar} \\times 1.5$ then $\\beta \\leftarrow \\beta \\times 2$ else pass This approach is simple to implement but you still have to choose a target value for $\\delta_{tar}$. In general it is computationally expensiv e to calculate the KL, which is one reason why the second form of the PPO is often preferred. Clipped Surrogate Objective The clipped surrogate objective is much simpler and omits the calculation of the KL entirely: $$ J^{CLIP}(\\theta) = \\mathbb{E}_{t} \\Big[ \\min \\big( r_{t}(\\theta) A_{t}, clip(r_{t}(\\theta), 1 - \\epsilon, 1 + \\epsilon)A_{t} \\big) \\Big] $$\n$\\epsilon$ is a hyperparameter which defines the clipping neighbourhood and can be decayed during training. This objective function prevents parameter updates which could cause large and risky changes to the policy $\\pi_{\\theta}$. As mentioned before, you can use both of the described objectives with e.g. Actor-Critic methods or REINFORCE. For details take a look at the paper itself. Figure 1. Algorithm - PPO Actor-Critic [2]\nDown below you can find a few results from the paper on several MuJoCo environments after one million steps of training. At that times it outperformed the previous methods on almost all the continuous control environments. Figure 2. MuJoCo environment results [2]\nAlpha(Go)Zero AlphaGo Zero is the successor of DeepMinds AlphaGo which was the first program to defeat a world champion in the game of Go. It differs from AlphaGo in several important aspects. It was trained solely by self-play without any use of expert data or human supervision, but knowing the rules of the game (model-based). It uses only a single neural network and defeated AlphaGo by 100 games to 0. With AlphaZero, DeepMind generalized and extended the method to the games Chess and Shogi. AlphaGo Zero combines a neural network and Monte Carlo Tree Search (MCTS) in an elegant policy iteration framework to achieve stable learning and rapid improvement. Lets start by taking a closer look at the neural network. Neural Network The authors used a deep Convolutional Neural Network $f_{\\theta}$ which is parameterized by $\\theta$ and takes as input the state $\\mathbf{s}$ of the board and outputs both, move probabilities and a value $(\\mathbf{p}, v) = f_{\\theta}(s)$. It combines the roles of the policy network and value network into a single architecture, similar to the shared network which can be used in Actor-Critic methods. The vector of the move probabilities $\\mathbf{p}$ represents the probability of selecting each move $a$, $\\mathbf{p}_{a} = Pr(a|s)$. The value $v$ is a scalar evaluation, estimating the probability of the current player winning from position $\\mathbf{s}$. The network is trained at the end of each game of self-play based on the data of each time step $t$ which is stored in the form $(s_{t}, \\pi_{t}, z_{t})$. $\\pi_{t}$ represents the search probabilities generated by MCTS and $z_{t} \\in {-1, 1}$ is the game winner from the perspective of the current player (gets added at the end of the game to each timestep). The loss function $l$ sumsof the mean-squared error and cross-entropy loss: $$ l = (z - v)^{2} - \\pi^{T} \\log \\mathbf{p} + c || \\theta ||^{2} $$\nwhere $c$ is a hyperparameter controlling the level of L2 weight regularization. Figure 3. down below illustrates this self-play RL procedure in AlphaGo Zero. Figure 3. AlphaGo Zero - Self-play and neural network training [7]\nMonte Carlo Tree Search The neural network we just talked about is trained from games of self-play by a novel reinforcement learning algorithm. In each position $s$, an MCTS is executed, guided by the neural network $f_{\\theta}$ and it outputs search probabilities $\\pi$ of playing each move. Based on these you can usually select a much stronger move than based on the raw move probabilities $\\mathbf{p}$ output by the neural network $f_{\\theta}(s)$ which makes MCTS to kind of a policy improvement operator. It uses the neural network $f_{\\theta}$ for its simulations. Each edge $(s,a)$ in the search tree stores a prior probability $P(s,a)$, a visit count $N(s,a)$ and an action value $Q(s,a)$. Each simulation run starts from the root state and iteratively selects moves that maximize an upper confidence bound $$ U(s,a) = Q(s,a) + c * P(s,a) * \\frac{\\sum_{b}N(s,b)}{1 + N(s,a)} $$\nwhere c is a hyperparameter which controls the degree of exploration. We initialize our empty search tree with $\\mathbf{s}$ as the root. A single simulation run proceeds as follows. First we compute the action $a$ that maximizes the upper confidence bound $U(s,a)$. We play this action and if the resulting state $\\mathbf{s'}$ exists in our tree, we recursively search on $\\mathbf{s'}$. If it doesn't exist, it gets added to the tree and $Q(s',a)$ and $N(s',a)$ are initialized to 0 for all $a$. The neural network is used to initialize $P(s', \\cdot) = \\mathbf{p}$ and $v(s') = v_{\\theta}(s')$. Instead of performing a rollout, $v(s')$ gets propagated up along the path seen in the current simulation and update all $Q(s,a)$ values. If on the other hand a terminal state gets encountered, the reward gets propagated. Figure 4. AlphaGo Zero - MCTS Search Process [7]\nDeepMind trained the AlphaGo Zero system for approximately three days, generating 4.9 million games of self-play, using 1,600 simulations for each MCTS. The neural net they used consisted of 20 residual blocks (they also trained a bigger network for longer). Summary In this post we looked at three combined RL methods, A2C, PPO and AlphaGoZero. As Alpha(Go)Zero is a more specialized method for games, PPO and A2C are more generally applicable. I hope I was able to give you an understandable insight into the three algorithms and their most important components. In the next post of this series I am going to dive a bit deeper into model-based RL and imitation learning. Until then, stay tuned and healthy! References [1] Mnih et al. “Asynchronous Methods for Deep Reinforcement Learning” (2016).\n[2] Schulman et al. “Proximal Policy Optimization Algorithms” (2017).\n[3] Kullback-Leiber Divergence Explained (2017).\n[4] Schulman et al. “Trust Region Policy Optimization” (2015).\n[5] Silver et al. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm” (2017).\n[6] Schulman et al. “High-Dimensional Continuous Control Using Generalized Advantage Estimation” (2015).\n[7] Silver et al. “Mastering the game of Go without human knowledge” (2017).\n","wordCount":"2609","inLanguage":"en","datePublished":"2022-03-14T10:00:00+02:00","dateModified":"2022-03-14T10:00:00+02:00","author":{"@type":"Person","name":"Johann Gerberding"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://johanngerberding.github.io/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/"},"publisher":{"@type":"Organization","name":"Johanns Blog","logo":{"@type":"ImageObject","url":"https://johanngerberding.github.io/favicon.ico"}}}</script><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  border-collapse: collapse;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-msubsup {
  display: inline-block;
  text-align: left;
}

mjx-script {
  display: inline-block;
  padding-right: .05em;
  padding-left: .033em;
}

mjx-script > mjx-spacer {
  display: block;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-mspace {
  display: inline-block;
  text-align: left;
}

mjx-mover {
  display: inline-block;
  text-align: left;
}

mjx-mover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-mover:not([limits="false"]) > * {
  display: block;
  text-align: left;
}

mjx-munderover {
  display: inline-block;
  text-align: left;
}

mjx-munderover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-munderover:not([limits="false"]) > * {
  display: block;
}

mjx-munder {
  display: inline-block;
  text-align: left;
}

mjx-over {
  text-align: left;
}

mjx-munder:not([limits="false"]) {
  display: inline-table;
}

mjx-munder > mjx-row {
  text-align: left;
}

mjx-under {
  padding-bottom: .1em;
}

mjx-mtable {
  display: inline-block;
  text-align: center;
  vertical-align: .25em;
  position: relative;
  box-sizing: border-box;
  border-spacing: 0;
  border-collapse: collapse;
}

mjx-mstyle[size="s"] mjx-mtable {
  vertical-align: .354em;
}

mjx-labels {
  position: absolute;
  left: 0;
  top: 0;
}

mjx-table {
  display: inline-block;
  vertical-align: -.5ex;
  box-sizing: border-box;
}

mjx-table > mjx-itable {
  vertical-align: middle;
  text-align: left;
  box-sizing: border-box;
}

mjx-labels > mjx-itable {
  position: absolute;
  top: 0;
}

mjx-mtable[justify="left"] {
  text-align: left;
}

mjx-mtable[justify="right"] {
  text-align: right;
}

mjx-mtable[justify="left"][side="left"] {
  padding-right: 0 ! important;
}

mjx-mtable[justify="left"][side="right"] {
  padding-left: 0 ! important;
}

mjx-mtable[justify="right"][side="left"] {
  padding-right: 0 ! important;
}

mjx-mtable[justify="right"][side="right"] {
  padding-left: 0 ! important;
}

mjx-mtable[align] {
  vertical-align: baseline;
}

mjx-mtable[align="top"] > mjx-table {
  vertical-align: top;
}

mjx-mtable[align="bottom"] > mjx-table {
  vertical-align: bottom;
}

mjx-mtable[side="right"] mjx-labels {
  min-width: 100%;
}

mjx-mtr {
  display: table-row;
  text-align: left;
}

mjx-mtr[rowalign="top"] > mjx-mtd {
  vertical-align: top;
}

mjx-mtr[rowalign="center"] > mjx-mtd {
  vertical-align: middle;
}

mjx-mtr[rowalign="bottom"] > mjx-mtd {
  vertical-align: bottom;
}

mjx-mtr[rowalign="baseline"] > mjx-mtd {
  vertical-align: baseline;
}

mjx-mtr[rowalign="axis"] > mjx-mtd {
  vertical-align: .25em;
}

mjx-mtd {
  display: table-cell;
  text-align: center;
  padding: .215em .4em;
}

mjx-mtd:first-child {
  padding-left: 0;
}

mjx-mtd:last-child {
  padding-right: 0;
}

mjx-mtable > * > mjx-itable > *:first-child > mjx-mtd {
  padding-top: 0;
}

mjx-mtable > * > mjx-itable > *:last-child > mjx-mtd {
  padding-bottom: 0;
}

mjx-tstrut {
  display: inline-block;
  height: 1em;
  vertical-align: -.25em;
}

mjx-labels[align="left"] > mjx-mtr > mjx-mtd {
  text-align: left;
}

mjx-labels[align="right"] > mjx-mtr > mjx-mtd {
  text-align: right;
}

mjx-mtd[extra] {
  padding: 0;
}

mjx-mtd[rowalign="top"] {
  vertical-align: top;
}

mjx-mtd[rowalign="center"] {
  vertical-align: middle;
}

mjx-mtd[rowalign="bottom"] {
  vertical-align: bottom;
}

mjx-mtd[rowalign="baseline"] {
  vertical-align: baseline;
}

mjx-mtd[rowalign="axis"] {
  vertical-align: .25em;
}

mjx-mfrac {
  display: inline-block;
  text-align: left;
}

mjx-frac {
  display: inline-block;
  vertical-align: 0.17em;
  padding: 0 .22em;
}

mjx-frac[type="d"] {
  vertical-align: .04em;
}

mjx-frac[delims] {
  padding: 0 .1em;
}

mjx-frac[atop] {
  padding: 0 .12em;
}

mjx-frac[atop][delims] {
  padding: 0;
}

mjx-dtable {
  display: inline-table;
  width: 100%;
}

mjx-dtable > * {
  font-size: 2000%;
}

mjx-dbox {
  display: block;
  font-size: 5%;
}

mjx-num {
  display: block;
  text-align: center;
}

mjx-den {
  display: block;
  text-align: center;
}

mjx-mfrac[bevelled] > mjx-num {
  display: inline-block;
}

mjx-mfrac[bevelled] > mjx-den {
  display: inline-block;
}

mjx-den[align="right"], mjx-num[align="right"] {
  text-align: right;
}

mjx-den[align="left"], mjx-num[align="left"] {
  text-align: left;
}

mjx-nstrut {
  display: inline-block;
  height: .054em;
  width: 0;
  vertical-align: -.054em;
}

mjx-nstrut[type="d"] {
  height: .217em;
  vertical-align: -.217em;
}

mjx-dstrut {
  display: inline-block;
  height: .505em;
  width: 0;
}

mjx-dstrut[type="d"] {
  height: .726em;
}

mjx-line {
  display: block;
  box-sizing: border-box;
  min-height: 1px;
  height: .06em;
  border-top: .06em solid;
  margin: .06em -.1em;
  overflow: hidden;
}

mjx-line[type="d"] {
  margin: .18em -.1em;
}

mjx-mrow {
  display: inline-block;
  text-align: left;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-c.mjx-c1D444.TEX-I::before {
  padding: 0.704em 0.791em 0.194em 0;
  content: "Q";
}

mjx-c.mjx-c1D70B.TEX-I::before {
  padding: 0.431em 0.57em 0.011em 0;
  content: "\3C0";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c1D449.TEX-I::before {
  padding: 0.683em 0.769em 0.022em 0;
  content: "V";
}

mjx-c.mjx-c1D434.TEX-I::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c1D703.TEX-I::before {
  padding: 0.705em 0.469em 0.01em 0;
  content: "\3B8";
}

mjx-c.mjx-c1D445.TEX-I::before {
  padding: 0.683em 0.759em 0.021em 0;
  content: "R";
}

mjx-c.mjx-c1D461.TEX-I::before {
  padding: 0.626em 0.361em 0.011em 0;
  content: "t";
}

mjx-c.mjx-c1D70F.TEX-I::before {
  padding: 0.431em 0.517em 0.013em 0;
  content: "\3C4";
}

mjx-c.mjx-c2207::before {
  padding: 0.683em 0.833em 0.033em 0;
  content: "\2207";
}

mjx-c.mjx-c1D43D.TEX-I::before {
  padding: 0.683em 0.633em 0.022em 0;
  content: "J";
}

mjx-c.mjx-c1D53C.TEX-A::before {
  padding: 0.683em 0.667em 0 0;
  content: "E";
}

mjx-c.mjx-c5B.TEX-S2::before {
  padding: 1.15em 0.472em 0.649em 0;
  content: "[";
}

mjx-c.mjx-c6C::before {
  padding: 0.694em 0.278em 0 0;
  content: "l";
}

mjx-c.mjx-c6F::before {
  padding: 0.448em 0.5em 0.01em 0;
  content: "o";
}

mjx-c.mjx-c67::before {
  padding: 0.453em 0.5em 0.206em 0;
  content: "g";
}

mjx-c.mjx-c2061::before {
  padding: 0 0 0 0;
  content: "";
}

mjx-c.mjx-c7C::before {
  padding: 0.75em 0.278em 0.249em 0;
  content: "|";
}

mjx-c.mjx-c5D.TEX-S2::before {
  padding: 1.15em 0.472em 0.649em 0;
  content: "]";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c39::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "9";
}

mjx-c.mjx-c223C::before {
  padding: 0.367em 0.778em 0 0;
  content: "\223C";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c1D6FE.TEX-I::before {
  padding: 0.441em 0.543em 0.216em 0;
  content: "\3B3";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c2026::before {
  padding: 0.12em 1.172em 0 0;
  content: "\2026";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c5E::before {
  padding: 0.694em 0.5em 0 0;
  content: "^";
}

mjx-c.mjx-c1D441.TEX-I::before {
  padding: 0.683em 0.888em 0 0;
  content: "N";
}

mjx-c.mjx-c1D446.TEX-I::before {
  padding: 0.705em 0.645em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c1D447.TEX-I::before {
  padding: 0.677em 0.704em 0 0;
  content: "T";
}

mjx-c.mjx-c1D438.TEX-I::before {
  padding: 0.68em 0.764em 0 0;
  content: "E";
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

mjx-c.mjx-c2248::before {
  padding: 0.483em 0.778em 0 0;
  content: "\2248";
}

mjx-c.mjx-c33::before {
  padding: 0.665em 0.5em 0.022em 0;
  content: "3";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c1D43A.TEX-I::before {
  padding: 0.705em 0.786em 0.022em 0;
  content: "G";
}

mjx-c.mjx-c221E::before {
  padding: 0.442em 1em 0.011em 0;
  content: "\221E";
}

mjx-c.mjx-c2211.TEX-S2::before {
  padding: 0.95em 1.444em 0.45em 0;
  content: "\2211";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D706.TEX-I::before {
  padding: 0.694em 0.583em 0.012em 0;
  content: "\3BB";
}

mjx-c.mjx-c1D6FF.TEX-I::before {
  padding: 0.717em 0.444em 0.01em 0;
  content: "\3B4";
}

mjx-c.mjx-c77::before {
  padding: 0.431em 0.722em 0.011em 0;
  content: "w";
}

mjx-c.mjx-c68::before {
  padding: 0.694em 0.556em 0 0;
  content: "h";
}

mjx-c.mjx-c65::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c72::before {
  padding: 0.442em 0.392em 0 0;
  content: "r";
}

mjx-c.mjx-cA0::before {
  padding: 0 0.25em 0 0;
  content: "\A0";
}

mjx-c.mjx-c3B::before {
  padding: 0.43em 0.278em 0.194em 0;
  content: ";";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c1D41A.TEX-B::before {
  padding: 0.453em 0.559em 0.006em 0;
  content: "a";
}

mjx-c.mjx-c1D42C.TEX-B::before {
  padding: 0.453em 0.454em 0.006em 0;
  content: "s";
}

mjx-c.mjx-c1D719.TEX-I::before {
  padding: 0.694em 0.596em 0.205em 0;
  content: "\3D5";
}

mjx-c.mjx-c2190::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2190";
}

mjx-c.mjx-c1D6FC.TEX-I::before {
  padding: 0.442em 0.64em 0.011em 0;
  content: "\3B1";
}

mjx-c.mjx-c1D43E.TEX-I::before {
  padding: 0.683em 0.889em 0 0;
  content: "K";
}

mjx-c.mjx-c1D43F.TEX-I::before {
  padding: 0.683em 0.681em 0 0;
  content: "L";
}

mjx-c.mjx-c6D::before {
  padding: 0.442em 0.833em 0 0;
  content: "m";
}

mjx-c.mjx-c61::before {
  padding: 0.448em 0.5em 0.011em 0;
  content: "a";
}

mjx-c.mjx-c78::before {
  padding: 0.431em 0.528em 0 0;
  content: "x";
}

mjx-c.mjx-c1D6FD.TEX-I::before {
  padding: 0.705em 0.566em 0.194em 0;
  content: "\3B2";
}

mjx-c.mjx-c28.TEX-S1::before {
  padding: 0.85em 0.458em 0.349em 0;
  content: "(";
}

mjx-c.mjx-c2225::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "\2225";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c29.TEX-S1::before {
  padding: 0.85em 0.458em 0.349em 0;
  content: ")";
}

mjx-c.mjx-c69::before {
  padding: 0.669em 0.278em 0 0;
  content: "i";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c3C::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: "<";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c3E::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: ">";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c1D436.TEX-I::before {
  padding: 0.705em 0.76em 0.022em 0;
  content: "C";
}

mjx-c.mjx-c1D43C.TEX-I::before {
  padding: 0.683em 0.504em 0 0;
  content: "I";
}

mjx-c.mjx-c6E::before {
  padding: 0.442em 0.556em 0 0;
  content: "n";
}

mjx-c.mjx-c1D450.TEX-I::before {
  padding: 0.442em 0.433em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D716.TEX-I::before {
  padding: 0.431em 0.406em 0.011em 0;
  content: "\3F5";
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

mjx-c.mjx-c1D429.TEX-B::before {
  padding: 0.45em 0.639em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D463.TEX-I::before {
  padding: 0.443em 0.485em 0.011em 0;
  content: "v";
}

mjx-c.mjx-c1D467.TEX-I::before {
  padding: 0.442em 0.465em 0.011em 0;
  content: "z";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c1D448.TEX-I::before {
  padding: 0.683em 0.767em 0.022em 0;
  content: "U";
}

mjx-c.mjx-c2217::before {
  padding: 0.465em 0.5em 0 0;
  content: "\2217";
}

mjx-c.mjx-c2211.TEX-S1::before {
  padding: 0.75em 1.056em 0.25em 0;
  content: "\2211";
}

mjx-c.mjx-c1D44F.TEX-I::before {
  padding: 0.694em 0.429em 0.011em 0;
  content: "b";
}

mjx-c.mjx-c22C5::before {
  padding: 0.31em 0.278em 0 0;
  content: "\22C5";
}
</style></head><body class="dark" id="top"><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class="header"><nav class="nav"><div class="logo"><a href="https://johanngerberding.github.io/" accesskey="h" title="Johanns Blog (Alt + H)">Сиденёв Кирилл</a>
<span class="logo-switches"><button id="theme-toggle" accesskey="t" title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"></path></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg></button></span></div><ul id="menu"><li><a href="https://johanngerberding.github.io/about-me/" title="about"><span><i class="fa fa-heart"></i>Обо мне</span></a></li><li><a href="https://johanngerberding.github.io/posts/" title="posts"><span><i class="fa fa-heart"></i>Публикации</span></a></li></ul></nav></header><main class="main"><article class="post-single"><header class="post-header"><h1 class="post-title">Путь, который стоило пройти, чтобы получить бесценный опыт</h1></header>

<p align="justify">Попытка создать приложение для нашего рынка обернулась для меня хорошим опытом, большим количеством потраченных часов и ценными связями. Могу ли я сказать, что рад этому - несомненно.</p><div class="social-icons">
    <img src="https://sun9-52.userapi.com/impg/LK3iHH-MgLOMPcUX4pyfgHFRvH5NZnOPHSP3pQ/EFtLyO9BSrU.jpg?size=1080x1080&amp;quality=95&amp;sign=187279044f9106fb45d3172d09f06d79&amp;type=album" alt="alt tag" data-canonical-src="https://sun9-60.userapi.com/c857632/v857632703/2334d5/whWQkSejldQ.jpg" style="max-width: 100%;border-radius: 8px;">
</div>
<div class="post-content"><p align="justify">Что я вынес из всего этого ? - "Всё что нужно" - это учиться на своих ошибках и не позволять им повторяться.

</p><br></div><footer class="post-footer"></footer></article></main><footer class="footer">
    <span>© 2022 GANG MEDIA GROUP</span>
  
</footer><a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g" style="visibility: hidden; opacity: 0;"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"></path></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
